{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Complementary notebook for finetuning a model. Refer to the full paper for a complete walkthrough.",
   "metadata": {
    "cell_id": "9e080f0d2eb64c80925425a87ff1fa0f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "ff719453-adb3-4e26-8de1-5fc19300e802",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d393753",
    "execution_start": 1650562030389,
    "execution_millis": 21935,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369.890625
   },
   "source": "import random\nimport torch\nimport csv\nimport spacy\nimport re\nimport json\n\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom tqdm import trange\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "415282d81ce445db90c08c8cf4a3b202",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d6b1316e",
    "execution_start": 1650536590432,
    "execution_millis": 30084,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 206.78125
   },
   "source": "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\", pad_token=\"<|endoftext|>\")\nconfiguration = GPT2Config.from_pretrained(\"distilgpt2\", output_hidden_states=True, output_attention=True)\nmodel = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", config=configuration)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 17.2MB/s]\nDownloading: 100%|██████████| 446k/446k [00:00<00:00, 14.5MB/s]\nDownloading: 100%|██████████| 762/762 [00:00<00:00, 164kB/s]\nDownloading: 100%|██████████| 336M/336M [00:26<00:00, 13.5MB/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Example output without finetuning",
   "metadata": {
    "cell_id": "480002f4de3f4c1d8976b3657d1d9e8f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0e19b2da3e184f4f871efbb9b79efdb3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b629f9cb",
    "execution_start": 1650531784580,
    "execution_millis": 1245,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 574.671875
   },
   "source": "input_ids = tokenizer.encode(\n    \"Neural network models can use attention mechanisms to direct their focus. This can help with\",\n    return_tensors=\"pt\",\n)\n\nsample_outputs = model.generate(\n    input_ids, \n    max_length=len(input_ids[0]) + 12,\n    do_sample=True, \n    top_k=50,\n    no_repeat_ngram_size=2, \n    top_p=0.8, \n    temperature=0.9,\n    num_return_sequences=3,\n    return_dict_in_output=True,\n    output_scores=True\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nOutput:\n----------------------------------------------------------------------------------------------------\n0: Neural network models can use attention mechanisms to direct their focus. This can help with the ability to predict the level of interest of a particular group\n1: Neural network models can use attention mechanisms to direct their focus. This can help with spatial memory and other learning mechanisms. The goal of this paper\n2: Neural network models can use attention mechanisms to direct their focus. This can help with the ability to identify and identify potential patterns in a variety of\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ebac00641b954fbd8be1c47b20b6db80",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8f4e45a4",
    "execution_start": 1650531599110,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "with open(\"./data.json\", \"r\") as read_file:\n    data = json.load(read_file)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "81f06af7f37a4eac92358812aea38c18",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9143073b",
    "execution_start": 1650531601652,
    "execution_millis": 1066,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 154.1875,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "max_length = max([len(tokenizer.encode(content)) for content in data]) + 2\n\nmax_length",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "172"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c7b4e2c0fe764b5d94a1b49f3aaea486",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6e16c13a",
    "execution_start": 1650527684143,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 603
   },
   "source": "import torch\n\ntorch.manual_seed(42)\nfrom torch.utils.data import Dataset\n\n\nclass FinetuneDataset(Dataset):\n    def __init__(self, data, tokenizer, gpt2_type=\"distilgpt2\", max_length=max_length):\n\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attn_masks = []\n\n        for content in data:\n            encodings_dict = tokenizer(\n                tokenizer.bos_token + content + tokenizer.eos_token,\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\",\n            )\n\n            self.input_ids.append(torch.tensor(encodings_dict[\"input_ids\"]))\n            self.attn_masks.append(torch.tensor(encodings_dict[\"attention_mask\"]))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00ab3f8b5f3d4efbbaed46ba0057c05d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fefccbe7",
    "execution_start": 1650527698310,
    "execution_millis": 1196,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 280.1875,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "from torch.utils.data import random_split\n\ndataset = FinetuneDataset(data, tokenizer, max_length=max_length)\n\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nf'There are {train_size} samples for training, and {val_size} samples for validation testing'",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 24,
     "data": {
      "text/plain": "'There are 1681 samples for training, and 187 samples for validation testing'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "696d7e731f32464098766ed51403b5e3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "401fcbff",
    "execution_start": 1650527706793,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 315
   },
   "source": "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbs = 32\ntrain_dataloader = DataLoader(\n            train_dataset,  \n            sampler = RandomSampler(train_dataset),\n            batch_size = bs\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, \n            sampler = SequentialSampler(val_dataset),\n            batch_size = bs \n        )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "fe518656d56843348f1f527cead27bba",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a295f602",
    "execution_start": 1650527720234,
    "execution_millis": 5,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 244.1875,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "import random\nimport numpy as np\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 26,
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f46889e58b0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "53eef39f99e4469e9d03adc096b7695e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "69bbf2b0",
    "execution_start": 1650527412503,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "epochs = 3\nwarmup_steps = 1e2\nsample_every = 100",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1bd016e2b4904a30a1b5c54d3530f06b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "335e063c",
    "execution_start": 1650527753265,
    "execution_millis": 8,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "from transformers import get_linear_schedule_with_warmup\nfrom transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-4, eps=1e-8)\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = warmup_steps, \n                                            num_training_steps = total_steps)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "fd1156d6f70446ebb3070f64de1414e5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "be34939e",
    "execution_start": 1650527759727,
    "execution_millis": 3260653,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2554.203125
   },
   "source": "import random\nimport time\nimport datetime\n\n\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n\n\ntotal_t0 = time.time()\n\ntraining_stats = []\n\nfor epoch_i in range(0, epochs):\n\n    print(f\"Beginning epoch {epoch_i + 1} of {epochs}\")\n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0]\n        b_labels = batch[0]\n        b_masks = batch[1]\n\n        model.zero_grad()\n\n        outputs = model(\n            b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None\n        )\n\n        loss = outputs[0]\n\n        batch_loss = loss.item()\n        total_train_loss += batch_loss\n\n        # Get sample every 100 batches.\n        if step % sample_every == 0 and not step == 0:\n\n            elapsed = format_time(time.time() - t0)\n            print(\n                f\"Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}\"\n            )\n\n            model.eval()\n\n            sample_outputs = model.generate(\n                bos_token_id=random.randint(1, 30000),\n                do_sample=True,\n                top_k=50,\n                max_length=200,\n                top_p=0.95,\n                num_return_sequences=1,\n            )\n            for i, sample_output in enumerate(sample_outputs):\n                print(\n                    f\"Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}\"\n                )\n\n            model.train()\n\n        loss.backward()\n\n        optimizer.step()\n\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(f\"Average Training Loss: {avg_train_loss}. Epoch time: {training_time}\")\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n\n        b_input_ids = batch[0]\n        b_labels = batch[0]\n        b_masks = batch[1]\n\n        with torch.no_grad():\n\n            outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)\n\n            loss = outputs[0]\n\n        batch_loss = loss.item()\n        total_eval_loss += batch_loss\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n\n    validation_time = format_time(time.time() - t0)\n\n    print(f\"Validation loss: {avg_val_loss}. Validation Time: {validation_time}\")\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            \"epoch\": epoch_i + 1,\n            \"Training Loss\": avg_train_loss,\n            \"Valid. Loss\": avg_val_loss,\n            \"Training Time\": training_time,\n            \"Validation Time\": validation_time,\n        }\n    )\n\nprint(f\"Total training took {format_time(time.time()-total_t0)}\")\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Beginning epoch 1 of 6\nAverage Training Loss: 2.480187913156905. Epoch time: 0:08:39\nValidation loss: 0.9046754837036133. Validation Time: 0:00:18\nBeginning epoch 2 of 6\nAverage Training Loss: 0.8645765084140705. Epoch time: 0:10:08\nValidation loss: 0.8574422101179758. Validation Time: 0:00:19\nBeginning epoch 3 of 6\nAverage Training Loss: 0.7577598690986633. Epoch time: 0:08:27\nValidation loss: 0.8441044290860494. Validation Time: 0:00:17\nBeginning epoch 4 of 6\nAverage Training Loss: 0.6670659690533044. Epoch time: 0:08:31\nValidation loss: 0.8529025316238403. Validation Time: 0:00:18\nBeginning epoch 5 of 6\nAverage Training Loss: 0.6000708830806444. Epoch time: 0:08:19\nValidation loss: 0.8662164111932119. Validation Time: 0:00:19\nBeginning epoch 6 of 6\nAverage Training Loss: 0.5509948156914621. Epoch time: 0:08:28\nValidation loss: 0.885182112455368. Validation Time: 0:00:18\nTotal training took 0:54:21\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7dd078dcfc5c48f3b8c5ae1cc6ef7587",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af8f8672",
    "execution_start": 1650531032944,
    "execution_millis": 1925,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 212.96875,
    "deepnote_output_heights": [
     97.96875
    ]
   },
   "source": "model.save_pretrained('./modelv2/')\ntokenizer.save_pretrained('./modelv2/')",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 31,
     "data": {
      "text/plain": "('./model/tokenizer_config.json',\n './model/special_tokens_map.json',\n './model/vocab.json',\n './model/merges.txt',\n './model/added_tokens.json')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e715a4c3cdfe4d28a0a7401f15898a5a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "67a8e25a",
    "execution_start": 1650531616426,
    "execution_millis": 4448,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "finetuned_tokenizer = GPT2Tokenizer.from_pretrained('./model/')\nfinetuned_model = GPT2LMHeadModel.from_pretrained('./model/')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Inferences from the fine-tuned model. Notice how the network has learned to capitalize keywords such as \"Neural Networks\" and \"Memory Systems\".",
   "metadata": {
    "cell_id": "89c15ebed59b469888d709a9bcc17653",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.796875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c47b7e62afb5428d95c89ad2dc404cce",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9405969e",
    "execution_start": 1650531742246,
    "execution_millis": 1321,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 574.671875
   },
   "source": "input_ids = finetuned_tokenizer.encode(\n    \"Neural network models can use attention mechanisms to direct their focus. This can help with\",\n    return_tensors=\"pt\",\n)\n\nsample_outputs = finetuned_model.generate(\n    input_ids, \n    max_length=len(input_ids[0]) + 12,\n    do_sample=True, \n    top_k=50,\n    no_repeat_ngram_size=2, \n    top_p=0.8, \n    temperature=0.9,\n    num_return_sequences=3,\n    return_dict_in_output=True,\n    output_scores=True\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nOutput:\n----------------------------------------------------------------------------------------------------\n0: Neural network models can use attention mechanisms to direct their focus. This can help with the interpretation of Neural Networks.\n1: Neural network models can use attention mechanisms to direct their focus. This can help with the index on Memory Systems.\n2: Neural network models can use attention mechanisms to direct their focus. This can help with Attentional Neural Networks to learn how to train a trained model\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "To run this model on the client, I've converted it to onnx with `python -m transformers.onnx --model ../model --framework --feature=causal-lm pt .`\n\nAll od the following code will later be converted to javscript with [`onnx-runtime-node`](https://www.npmjs.com/package/onnxruntime-node).",
   "metadata": {
    "cell_id": "d748638aed0d49bab6b8b67d44cd05ce",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 111.1875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "dbb1d600110f4a3eb9ef4f1243179012",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c8976ca5",
    "execution_start": 1650562101898,
    "execution_millis": 8330,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "from onnxruntime import InferenceSession\nsession = InferenceSession(\"onnx/model.onnx\")\nfinetuned_tokenizer = GPT2Tokenizer.from_pretrained('./model/')",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1ccdd24c75694ebf9d897e0d25e5fc3e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6a47708c",
    "execution_start": 1650564342575,
    "execution_millis": 7,
    "owner_user_id": "b8b7d390-3d6b-4230-aef2-2c1c0bc7f6aa",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1755
   },
   "source": "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        sorted_indices_to_remove = cumulative_probs > top_p\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n    return logits\n\ndef normalize(data):\n    return (data - np.min(data)) / (np.max(data) - np.min(data))\n\ndef generate(\n    model,\n    tokenizer,\n    prompt,\n    max_length=16,\n    temperature=0.9,\n    repetition_penalty=0.8,\n    top_k=50,\n    top_p=0.9,\n    max_context_length=1024,\n):\n    with torch.no_grad():\n        break_tokens = [764, 50256, 198, 13]\n\n        generated = tokenizer(prompt, return_tensors=\"pt\")\n\n        new_tokens = generated[\"input_ids\"][0]\n        new_logits = []\n        temperature = temperature\n        repetition_penalty = repetition_penalty\n        top_k = top_k\n        top_p = top_p\n\n\n        for _ in trange(max_length):\n            outputs = model.run(\n                None,\n                input_feed={\n                    \"input_ids\": generated[\"input_ids\"].cpu().numpy(),\n                    \"attention_mask\": generated[\"attention_mask\"].cpu().numpy(),\n                },\n            )\n\n            logits = torch.tensor(outputs[0][0])\n            attention = outputs[-1][0]\n\n            next_token_logits = logits[-1, :] / (\n                temperature if temperature > 0 else 1.0\n            )\n\n            new_logits.append(next_token_logits)\n            for _ in set(generated[\"input_ids\"].view(-1).tolist()):\n                next_token_logits[_] /= repetition_penalty\n            if temperature == 0:  # greedy sampling:\n                next_token = torch.argmax(next_token_logits).unsqueeze(0)\n            else:\n                filtered_logits = top_k_top_p_filtering(\n                    next_token_logits, top_k=top_k, top_p=top_p\n                )\n                next_token = torch.multinomial(\n                    F.softmax(filtered_logits, dim=-1), num_samples=1\n                )\n\n            generated[\"input_ids\"] = torch.cat(\n                (generated[\"input_ids\"], next_token.unsqueeze(0)), dim=1\n            )\n            generated[\"attention_mask\"] = torch.cat(\n                (generated[\"attention_mask\"], torch.tensor([1]).unsqueeze(0)), dim=1\n            )\n            new_tokens = torch.cat((new_tokens, next_token), 0)\n\n            next_token_logit = next_token.item()\n            if next_token_logit in break_tokens:\n                break\n\n        normalized_attentions = normalize(np.mean(attention, axis=(1)))\n        print(\"\\n\\nNormalized attentions\\n---\")\n        for i, a in enumerate(normalized_attentions):\n            print(finetuned_tokenizer.decode(new_tokens[i]).strip(), a)\n\n        return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3f0977a82cde4653bd966b2cc1a076a6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6b47391e",
    "execution_start": 1650564546000,
    "execution_millis": 1702,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 825.1875,
    "deepnote_output_heights": [
     309.125,
     21.1875
    ]
   },
   "source": "input = \"Neural network models can use attention mechanisms to direct their focus. This can help with\"\noutput = generate(session, finetuned_tokenizer, input)\n\noutput\n",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "text": " 88%|████████▊ | 14/16 [00:01<00:00,  8.57it/s]\n\nNormalized attentions\n---\nNe 0.39964992\nural 0.5156584\nnetwork 0.6587709\nmodels 0.9730898\ncan 0.0834508\nuse 0.10494143\nattention 0.73390186\nmechanisms 0.8171208\nto 0.22891888\ndirect 0.4344021\ntheir 0.22744419\nfocus 0.36719105\n. 1.0\nThis 0.88731384\ncan 0.22770312\nhelp 0.0\nwith 0.033685226\nthe 0.07412371\nAttention 0.7709657\nDistribution 0.50096774\nMap 0.7513429\n, 0.5973961\nwhich 0.749522\nshows 0.11179085\nhow 0.20706418\nactivation 0.2883157\nof 0.2310062\nthese 0.43793496\nneural 0.43947598\nnetworks 0.55677927\nis 0.7408395\n\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 90,
     "data": {
      "text/plain": "'Neural network models can use attention mechanisms to direct their focus. This can help with the Attention Distribution Map, which shows how activation of these neural networks is.'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ad10b37e-1254-49b1-9814-3334468ab840' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "aeae7b27-82bb-45bb-a513-e795d1371f79",
  "deepnote_execution_queue": []
 }
}